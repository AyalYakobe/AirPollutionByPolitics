{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Pollution by Politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, explained_variance_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "from kneed import KneeLocator\n",
    "\n",
    "sys.path.append('../etc')\n",
    "from refs import state_abbreviation_dict\n",
    "\n",
    "# Optional (only if you truly need them at runtime)\n",
    "!pip install kneed\n",
    "!pip install -q xlrd openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `data` folder is approximately **14 GB**, it has been uploaded to a **shared Google Drive folder**.\n",
    "To access and use this data in this notebook, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Upload this notebook to **Google Colab**\n",
    "- Open [Google Colab](https://colab.research.google.com/).\n",
    "- Click on **File → Upload notebook** and select this notebook.\n",
    "\n",
    "### 2. Mount your Google Drive\n",
    "Insert and run the following code in a new Colab cell:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "- This will prompt you to authorize access to your Google Drive.\n",
    "\n",
    "### 3. Create a Shortcut of the `data` Folder in Your Google Drive\n",
    "- Open the shared Google Drive link.\n",
    "- **Right-click** the `data` folder → **Add shortcut to Drive**.\n",
    "- Choose a location inside your **MyDrive** (e.g., directly under \"MyDrive\" or inside a project folder).\n",
    "\n",
    "### 4. Update the File Paths in the Notebook\n",
    "When reading the data files, adjust the paths in your code to point to your shortcut location.\n",
    "For example:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example\n",
    "presidential_data = pd.read_csv('/content/drive/MyDrive/your_folder_name/data/election/relevant/usa/1976-2020-president.csv')\n",
    "```\n",
    "\n",
    "Replace `your_folder_name` with the actual folder name where you added the shortcut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presidential_data = pd.read_csv('/content/drive/MyDrive/School/data/election/relevant/usa/1976-2020-president.csv')\n",
    "# senate_data = pd.read_csv('/content/drive/MyDrive/School/data/election/relevant/usa/1976-2020-senate.csv')\n",
    "# house_data = pd.read_csv('/content/drive/MyDrive/School/data/election/relevant/usa/1976-2022-house.csv')\n",
    "# house_data['party_simplified'] = house_data['party'].apply(lambda x: x if x in ['DEMOCRAT', 'REPUBLICAN'] else 'OTHER')\n",
    "#\n",
    "# state_emissions_data = pd.read_excel(\n",
    "#     \"/content/drive/MyDrive/School/data/co2_emissions/relevant/1970-2022-state_emissions.xlsx\",\n",
    "#     engine=\"openpyxl\",\n",
    "#     skiprows=4\n",
    "# )\n",
    "#\n",
    "# state_population_size_data = pd.read_csv('/content/drive/MyDrive/School/data/general_state_data/population_data.csv')\n",
    "# state_population_size_data.rename(columns={'AK': 'state', '1950': 'year', '135000': 'population'}, inplace=True)\n",
    "# state_land_size_data = pd.read_csv('/content/drive/MyDrive/School/data/general_state_data/state_sizes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidential_data = pd.read_csv('../data/election/relevant/usa/1976-2020-president.csv')\n",
    "senate_data = pd.read_csv('../data/election/relevant/usa/1976-2020-senate.csv')\n",
    "house_data = pd.read_csv('../data/election/relevant/usa/1976-2022-house.csv')\n",
    "house_data['party_simplified'] = house_data['party'].apply(lambda x: x if x in ['DEMOCRAT', 'REPUBLICAN'] else 'OTHER')\n",
    "\n",
    "state_emissions_data = pd.read_excel(\n",
    "    \"../data/co2_emissions/relevant/1970-2022-state_emissions.xlsx\",\n",
    "    engine=\"openpyxl\",\n",
    "    skiprows=4\n",
    ")\n",
    "\n",
    "state_population_size_data = pd.read_csv('../data/general_state_data/population_data.csv')\n",
    "state_population_size_data.rename(columns={'AK': 'state', '1950': 'year', '135000': 'population'}, inplace=True)\n",
    "state_land_size_data = pd.read_csv('../data/general_state_data/state_sizes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis Per Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Election Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates a political score for each U.S. state based on presidential, House, and Senate election data. It processes the election data for each year, calculates political scores by considering factors like vote percentage, longest streak of party wins, and the party of the most recent winner. It then combines these scores using weighted averages for each state and year. The final political scores are scaled and adjusted using the `MinMaxScaler`, and the scores are returned in a DataFrame, where each row represents a state’s political score for a specific year. This method provides insights into state-level political trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores_by_year(presidential_data, house_data, senate_data):\n",
    "    years = sorted(presidential_data['year'].unique())\n",
    "    all_scores = []\n",
    "\n",
    "    for year in years:\n",
    "        pres_results = calculate_political_scores(presidential_data, year)\n",
    "        house_results = calculate_political_scores(house_data, year)\n",
    "        sen_results = calculate_political_scores(senate_data, year)\n",
    "        final_political_score = merge_and_average_scores(house_results, sen_results, pres_results, year)\n",
    "        all_scores.append(final_political_score)\n",
    "\n",
    "    return pd.concat(all_scores).reset_index(drop=True)\n",
    "\n",
    "def calculate_political_scores(df, year=2020):\n",
    "    df = df[df['year'] <= year].copy()  # Ensure a full copy is made\n",
    "    df_filtered = df[df['party_simplified'].isin(['DEMOCRAT', 'REPUBLICAN'])].copy()\n",
    "\n",
    "    df_filtered.loc[:, 'vote_percent'] = df_filtered['candidatevotes'] / df_filtered['totalvotes'] * 100\n",
    "\n",
    "    df_winner = df_filtered.loc[df_filtered.groupby(['year', 'state'])['candidatevotes'].idxmax()].copy()\n",
    "\n",
    "    state_streaks = {}\n",
    "    for state in df_winner['state'].unique():\n",
    "        state_data = df_winner[df_winner['state'] == state].sort_values('year')\n",
    "        current_party = None\n",
    "        streak = 0\n",
    "        max_streak = 0\n",
    "        last_winner = None\n",
    "\n",
    "        for _, row in state_data.iterrows():\n",
    "            if row['party_simplified'] == current_party:\n",
    "                streak += 1\n",
    "            else:\n",
    "                current_party = row['party_simplified']\n",
    "                streak = 1\n",
    "            max_streak = max(max_streak, streak)\n",
    "            last_winner = current_party\n",
    "\n",
    "        state_streaks[state] = {'longest_streak': max_streak, 'last_winner': last_winner}\n",
    "\n",
    "    df_summary = df_winner.groupby('state').agg(\n",
    "        total_elections=('year', 'count'),\n",
    "        avg_vote_percent=('vote_percent', 'mean'),\n",
    "        last_winner=('party_simplified', 'last'),\n",
    "        last_vote_percent=('vote_percent', 'last')\n",
    "    ).reset_index()\n",
    "\n",
    "    df_summary['longest_streak'] = df_summary['state'].map(lambda x: state_streaks[x]['longest_streak'])\n",
    "    df_summary['streak_party'] = df_summary['state'].map(lambda x: state_streaks[x]['last_winner'])\n",
    "\n",
    "    df_summary['party_numeric'] = df_summary['last_winner'].map({'REPUBLICAN': -1, 'DEMOCRAT': 1})\n",
    "\n",
    "    features = df_summary[['longest_streak', 'avg_vote_percent', 'last_vote_percent']]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    df_summary.loc[:, ['streak_score', 'avg_vote_score', 'last_vote_score']] = scaler.fit_transform(features)\n",
    "\n",
    "    df_summary['final_score'] = (df_summary['streak_score'] * 0.4 +\n",
    "                                 df_summary['avg_vote_score'] * 0.3 +\n",
    "                                 df_summary['last_vote_score'] * 0.3) * df_summary['party_numeric']\n",
    "    df_summary['year'] = year\n",
    "    return df_summary[['state', 'year', 'final_score']]\n",
    "\n",
    "def merge_and_average_scores(house_results, sen_results, pres_results, year=2020):\n",
    "    df_combined = house_results.merge(sen_results, on=['state', 'year'], suffixes=('_house', '_sen')) \\\n",
    "                               .merge(pres_results, on=['state', 'year'], suffixes=('', '_pres'))\n",
    "\n",
    "    df_combined['final_score_avg'] = df_combined[['final_score_house', 'final_score_sen', 'final_score']].mean(axis=1)\n",
    "\n",
    "    return df_combined[['state', 'year', 'final_score_avg']]\n",
    "\n",
    "final_political_scores_by_year = calculate_scores_by_year(presidential_data, house_data, senate_data)\n",
    "final_political_scores_by_year = final_political_scores_by_year.rename(columns={'final_score_avg':'party_affiliation_score'})\n",
    "final_political_scores_by_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emissions Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code analyzes state-level emissions data by excluding any rows containing \"Total\" in the state name. It then calculates the average emissions for each state across the available years and identifies the top 10 states with the highest and lowest average emissions. The results are visualized in two separate bar charts: one for the top 10 states with the highest emissions, and another for the top 10 states with the lowest emissions, using red and green colors, respectively. These plots help to easily identify and compare the states based on their average emissions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_emissions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_emissions_data = state_emissions_data[~state_emissions_data['State'].str.contains(\"Total\", case=False, na=False)]\n",
    "year_columns = [col for col in state_emissions_data.columns if isinstance(col, int)]\n",
    "top_states = state_emissions_data.set_index('State')[year_columns].mean(axis=1).nlargest(10)\n",
    "bottom_states = state_emissions_data.set_index('State')[year_columns].mean(axis=1).nsmallest(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_states.plot(kind='bar', color='red', alpha=0.7)\n",
    "plt.title(\"Top 10 States with Highest Yearly Average Emissions(measured in million metric tons of carbon dioxide)\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Average Emissions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bottom_states.plot(kind='bar', color='green', alpha=0.7)\n",
    "plt.title(\"Top 10 States with Lowest Yearly Average Emissions (measured in million metric tons of carbon dioxide)\")\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Average Emissions\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code analyzes and visualizes emissions data alongside political scores for U.S. states. It first filters out rows containing \"Total\" in the state name and then identifies the top and bottom 10 states based on their average emissions over time. For each of these states, it detects local minima and maxima in emissions using the `argrelextrema` function. It then plots emissions data over time, marking the local extrema points with blue (minima) and red (maxima) markers, while displaying corresponding political scores at each point. The `plot_emissions_with_extrema` function is used to generate two separate plots: one for the top 10 states with the highest emissions and another for the bottom 10 states with the lowest emissions. Additionally, the `extract_state_name` function extracts the state name from a given directory path. This comprehensive analysis helps in understanding the trends in emissions in relation to political scores over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_state_name(base_directory):\n",
    "    return os.path.basename(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_plot_emissions(state_emissions_data, political_scores_data):\n",
    "    state_emissions_data = state_emissions_data[~state_emissions_data['State'].str.contains(\"Total\", case=False, na=False)]\n",
    "    state_emissions_data = state_emissions_data[~state_emissions_data['State'].isna()]\n",
    "    state_emissions_data = state_emissions_data[~state_emissions_data['State'].str.contains(\"Source:\", na=False)]\n",
    "\n",
    "    year_columns = [col for col in state_emissions_data.columns if isinstance(col, int)]\n",
    "    top_states = state_emissions_data.set_index('State')[year_columns].mean(axis=1).nlargest(5).index\n",
    "    bottom_states = state_emissions_data.set_index('State')[year_columns].mean(axis=1).nsmallest(5).index\n",
    "\n",
    "    top_states_data = state_emissions_data[state_emissions_data['State'].isin(top_states)].set_index('State')[year_columns].T\n",
    "    bottom_states_data = state_emissions_data[state_emissions_data['State'].isin(bottom_states)].set_index('State')[year_columns].T\n",
    "\n",
    "    def find_local_extrema(data):\n",
    "        extrema = {}\n",
    "        for state in data.columns:\n",
    "            emissions = data[state].values\n",
    "            years = data.index.values\n",
    "            local_min_idx = argrelextrema(emissions, np.less)[0]  # Indices of local minima\n",
    "            local_max_idx = argrelextrema(emissions, np.greater)[0]  # Indices of local maxima\n",
    "            extrema[state] = {\n",
    "                \"min\": [(years[i], emissions[i]) for i in local_min_idx],  # Store (year, emission) pairs for mins\n",
    "                \"max\": [(years[i], emissions[i]) for i in local_max_idx]   # Store (year, emission) pairs for max\n",
    "            }\n",
    "        return extrema\n",
    "\n",
    "    top_extrema = find_local_extrema(top_states_data)\n",
    "    bottom_extrema = find_local_extrema(bottom_states_data)\n",
    "\n",
    "    def plot_emissions_with_extrema(data, extrema, title, scores_data):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        for state in data.columns:\n",
    "            plt.plot(data.index, data[state], label=state)\n",
    "\n",
    "            for x, y in extrema[state][\"min\"]:\n",
    "                plt.scatter(x, y, color='blue', marker='v', zorder=5)\n",
    "                # Convert both state names to uppercase for comparison\n",
    "                score = scores_data[(scores_data['state'].str.upper() == state.upper()) & (scores_data['year'] == x)]['party_affiliation_score'].values\n",
    "                if len(score) > 0:\n",
    "                    # Display the Min label with party affiliation score\n",
    "                    plt.text(x, y - 0.2, f\"Min {score[0]:.2f}\", fontsize=9, verticalalignment='bottom', horizontalalignment='center')\n",
    "            for x, y in extrema[state][\"max\"]:\n",
    "                plt.scatter(x, y, color='red', marker='^', zorder=5)\n",
    "                score = scores_data[(scores_data['state'].str.upper() == state.upper()) & (scores_data['year'] == x)]['party_affiliation_score'].values\n",
    "                if len(score) > 0:\n",
    "                    plt.text(x, y + 0.2, f\"Max {score[0]:.2f}\", fontsize=9, verticalalignment='top', horizontalalignment='center')\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Year\")\n",
    "        plt.ylabel(\"Emissions\")\n",
    "        plt.legend(title=\"States\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Move legend outside the plot\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_emissions_with_extrema(top_states_data, top_extrema, \"Top 5 States with Highest Emissions Over Time\", political_scores_data)\n",
    "    plot_emissions_with_extrema(bottom_states_data, bottom_extrema, \"Top 5 States with Lowest Emissions Over Time\", political_scores_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_emissions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_and_plot_emissions(state_emissions_data, final_political_scores_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic and Size Additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs data transformation and merging to create a comprehensive dataset for analysis. It starts by melting the `state_emissions_data` to reshape it, converting column names to lowercase and standardizing state names using a dictionary (`state_abbreviation_dict`). It then filters the data to include only valid years and merges it with `state_population_size_data` based on state and year. The `state_land_size_data` is also processed and merged to include land size information. Afterward, the political score data (`final_political_scores_by_year`) is integrated into the dataset by standardizing state names and performing another merge. The final result is a merged dataset (`final_merged_dataset`) containing emissions, population size, land size, and political scores for each state and year, ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_emissions_melted = state_emissions_data.melt(id_vars=[\"State\"],\n",
    "                                                   var_name=\"Year\",\n",
    "                                                   value_name=\"Emissions\")\n",
    "state_emissions_melted.columns = state_emissions_melted.columns.str.lower()\n",
    "state_emissions_melted = state_emissions_melted.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "state_emissions_melted[\"state\"] = state_emissions_melted[\"state\"].map(state_abbreviation_dict)\n",
    "\n",
    "state_emissions_melted[\"year\"] = state_emissions_melted[\"year\"].astype(str)\n",
    "state_emissions_melted = state_emissions_melted.dropna(subset=[\"year\"])  # Remove NaNs\n",
    "state_emissions_melted = state_emissions_melted[state_emissions_melted[\"year\"].str.isnumeric()]  # Keep only numeric years\n",
    "\n",
    "state_emissions_melted[\"year\"] = state_emissions_melted[\"year\"].astype(int)\n",
    "state_population_size_data[\"year\"] = state_population_size_data[\"year\"].astype(int)\n",
    "\n",
    "merged_population_and_emission_data = state_emissions_melted.merge(state_population_size_data, on=[\"state\", \"year\"], how=\"inner\")\n",
    "\n",
    "state_land_size_data = state_land_size_data.rename(columns={\"State\": \"state\"})\n",
    "state_land_size_data[\"state\"] = state_land_size_data[\"state\"].str.lower().map(state_abbreviation_dict)\n",
    "\n",
    "merged_size_population_emissions_data = merged_population_and_emission_data.merge(state_land_size_data, on=\"state\", how=\"inner\")\n",
    "merged_size_population_emissions_data = merged_size_population_emissions_data.rename(columns={\"Size (Square Miles)\": \"size\"})\n",
    "\n",
    "final_political_scores_by_year[\"state\"] = final_political_scores_by_year[\"state\"].str.lower().map(state_abbreviation_dict)\n",
    "\n",
    "\n",
    "\n",
    "all_states = final_political_scores_by_year['state'].unique()\n",
    "all_years = np.arange(1970, final_political_scores_by_year['year'].max() + 1)\n",
    "full_index = pd.MultiIndex.from_product([all_states, all_years], names=['state', 'year'])\n",
    "final_political_scores_full = final_political_scores_by_year.set_index(['state', 'year']).reindex(full_index).reset_index()\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "numeric_cols = final_political_scores_full.select_dtypes(include=[np.number]).columns\n",
    "final_political_scores_full[numeric_cols] = imputer.fit_transform(final_political_scores_full[numeric_cols])\n",
    "\n",
    "final_merged_dataset = merged_size_population_emissions_data.merge(\n",
    "    final_political_scores_full,\n",
    "    on=[\"state\", \"year\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "final_merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bills Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes environmental bills and votes data across multiple directories to calculate a \"Final Score\" for each state and year. It begins by loading JSON files for bills and votes, normalizing and merging the data, and calculating the percentage of \"yes\" votes for each bill. The code identifies climate-related bills by checking for specific environmental terms in the bill description, calculates a weighted climate score based on vote percentages, and sums these scores to get the final score. The code is designed to process files for all states and years in a directory structure, aggregating the results into a final DataFrame. It also measures the execution time and saves the results to a CSV file. The final output provides a dataset of average \"Final Scores\" by year and state, reflecting the states' environmental legislative activity over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bills_and_votes(bill_directory, vote_directory, environmental_terms):\n",
    "    def load_json_files_from_directory(directory_path):\n",
    "        data_list = []\n",
    "        if os.path.exists(directory_path) and os.listdir(directory_path):\n",
    "            for filename in os.listdir(directory_path):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    with open(os.path.join(directory_path, filename), 'r') as file:\n",
    "                        data = json.load(file)\n",
    "                        df = pd.json_normalize(data)\n",
    "                        data_list.append(df)\n",
    "            return pd.concat(data_list, ignore_index=True) if data_list else pd.DataFrame()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    bill_df = load_json_files_from_directory(bill_directory)\n",
    "    if bill_df.empty:\n",
    "        return 0\n",
    "\n",
    "    bill_df.columns = [col.split('.')[-1] for col in bill_df.columns]\n",
    "\n",
    "    vote_df = load_json_files_from_directory(vote_directory)\n",
    "    if vote_df.empty:\n",
    "        return 0\n",
    "\n",
    "    vote_df.columns = [col.split('.')[-1] for col in vote_df.columns]\n",
    "\n",
    "    merged_df = pd.merge(bill_df, vote_df, on=\"bill_id\", how=\"outer\")\n",
    "    merged_df = merged_df.drop_duplicates(subset='bill_id', keep='first')\n",
    "    merged_df['percent_yes'] = merged_df['yea'] / merged_df['total']\n",
    "    merged_df = merged_df[['percent_yes', 'bill_id', 'title', 'description']].reset_index().drop(columns=['index'])\n",
    "\n",
    "    merged_df['climate'] = merged_df['description'].apply(lambda x: 1 if any(term.lower() in str(x).lower() for term in environmental_terms) else 0)\n",
    "    merged_df.fillna(0)\n",
    "\n",
    "    climate_ratio = merged_df['climate'].sum() / len(merged_df) if len(merged_df) > 0 else 0\n",
    "    merged_df['weighted_climate'] = climate_ratio * merged_df['percent_yes']\n",
    "    final_score = merged_df['weighted_climate'].sum()\n",
    "\n",
    "    return final_score\n",
    "\n",
    "def process_all_files(base_directory, environmental_terms):\n",
    "    year = os.path.basename(base_directory).split('-')[0]\n",
    "    file_dict = {}\n",
    "\n",
    "    vote_dir = os.path.join(base_directory, 'vote')\n",
    "    bill_dir = os.path.join(base_directory, 'bill')\n",
    "\n",
    "    file_dict[year] = process_bills_and_votes(bill_dir, vote_dir, environmental_terms)\n",
    "\n",
    "    df = pd.DataFrame(list(file_dict.items()), columns=['Year', 'Final_Score'])\n",
    "    return df\n",
    "\n",
    "def process_all_states(base_directory, environmental_terms):\n",
    "    results = []\n",
    "    for subdir in os.listdir(base_directory):\n",
    "        subdir_path = os.path.join(base_directory, subdir)\n",
    "        print(subdir_path)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            subdirectory_df = process_all_files(subdir_path, environmental_terms)\n",
    "            results.append(subdirectory_df)\n",
    "\n",
    "    state = extract_state_name(base_directory)\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    final_df['State'] = state\n",
    "    return final_df\n",
    "\n",
    "def process_all_directories(base_directory, environmental_terms):\n",
    "    all_results = []\n",
    "    state_count = 1\n",
    "\n",
    "    for state_dir in os.listdir(base_directory):\n",
    "        print(f\"{state_count}: Processing state: {state_dir}\")\n",
    "        state_count+=1\n",
    "        state_dir_path = os.path.join(base_directory, state_dir)\n",
    "\n",
    "        if os.path.isdir(state_dir_path):\n",
    "            state_df = process_all_states(state_dir_path, environmental_terms)\n",
    "            all_results.append(state_df)\n",
    "\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    final_df = final_df.sort_values('Year').reset_index().drop(columns=['index'])\n",
    "    final_df = final_df.groupby(['Year', 'State'])['Final_Score'].mean().reset_index()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def process_all_directories_timed(base_directory, environmental_terms):\n",
    "    start_time = time.time()\n",
    "\n",
    "    results_df = process_all_directories(base_directory, environmental_terms)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    results_df.to_csv('data/results/election_scores.csv')\n",
    "    print(f\"Execution Time: {execution_time} seconds\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN - TAKES HOURS TO TERMINATE\n",
    "# results_df = process_all_directories_timed('data/bills', environmental_terms)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes the election scores data by reading a CSV file and cleaning the data by removing unnecessary columns and sorting by year and state. It then maps state abbreviations using a dictionary (`state_abbreviation_dict`). The data is grouped into four-year periods (starting from 2008), and the policy scores for each state are aggregated by summing the \"Final_Score\" within each year group. The resulting DataFrame is renamed to \"Policy Scores\" and contains aggregated scores by year group and state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_scores_df = pd.read_csv('../data/results/election_scores.csv')\n",
    "election_scores_df = election_scores_df.drop(columns=['Unnamed: 0'])\n",
    "election_scores_df = election_scores_df.sort_values(by=['Year', 'State'], ascending=True)\n",
    "election_scores_df['State'] = election_scores_df['State'].map(state_abbreviation_dict)\n",
    "\n",
    "start_year = 2008\n",
    "election_scores_df['Year_Group'] = (election_scores_df['Year'] - start_year) // 4 * 4 + start_year\n",
    "aggregated_scores_df = election_scores_df.groupby(['Year_Group', 'State'], as_index=False)['Final_Score'].sum()\n",
    "aggregated_scores_df = aggregated_scores_df.rename(columns={'Final_Score': 'Policy Scores'})\n",
    "aggregated_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code merges the `aggregated_scores_df` with `final_merged_dataset` based on matching year groups and states, using a right join to ensure all records from `final_merged_dataset` are included. It then drops the columns \"State\" and \"Year_Group\" and resets the index, cleaning up the final DataFrame. The resulting `final_df` contains the combined data with the aggregated policy scores and corresponding values from the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = aggregated_scores_df.merge(final_merged_dataset,\n",
    "                                       left_on=['Year_Group', 'State'],\n",
    "                                       right_on=['year', 'state'],\n",
    "                                       how='right')\n",
    "final_df = final_df.drop(columns=['State', 'Year_Group']).reset_index().drop(columns='index')\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code identifies the numerical and categorical columns in the `final_df` DataFrame by selecting columns with `float64` and `int64` data types for numerical columns, and excluding them for categorical columns. It then applies a K-Nearest Neighbors (KNN) imputer to fill any missing values in the numerical columns, using 5 neighbors to estimate the missing data. The imputed values are applied to a copy of `final_df`, resulting in a new DataFrame (`final_df_imputed`) with the missing numerical data filled in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = final_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = final_df.select_dtypes(exclude=['float64', 'int64']).columns\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "final_df_imputed = final_df.copy()\n",
    "final_df_imputed[numerical_columns] = imputer.fit_transform(final_df_imputed[numerical_columns])\n",
    "\n",
    "final_df_dropped = final_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a copy of the `final_df_imputed` DataFrame and applies standardization to the selected columns. It first identifies the columns to normalize by excluding \"state,\" \"year,\" and \"emissions\" from the list of columns. Then, it uses the `StandardScaler` from `sklearn.preprocessing` to normalize these columns, transforming them to have a mean of 0 and a standard deviation of 1. The transformed data is assigned back to the corresponding columns in the `final_df_imputed` DataFrame, resulting in the normalized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df, exclude_columns=[\"state\", \"year\", \"emissions\", \"party_affiliation_score\"]):\n",
    "    scaler = StandardScaler()\n",
    "    columns_to_normalize = [col for col in df.columns if col not in exclude_columns]\n",
    "    df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "    return df\n",
    "\n",
    "final_df_imputed_normalized = normalize_data(final_df_imputed)\n",
    "final_df_dropped_normalized = normalize_data(final_df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_emissions_melted = state_emissions_melted.dropna()\n",
    "state_emissions_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_imputed_normalized = final_df_imputed_normalized.copy()\n",
    "final_df_imputed_normalized.set_index(['state', 'year'], inplace=True)\n",
    "state_emissions_melted.set_index(['state', 'year'], inplace=True)\n",
    "\n",
    "final_df_imputed_normalized['emissions'] = state_emissions_melted['emissions']\n",
    "\n",
    "final_df_imputed_normalized.reset_index(inplace=True)\n",
    "state_emissions_melted.reset_index(inplace=True)\n",
    "final_df_imputed_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_dropped_normalized = final_df_dropped_normalized.copy()\n",
    "final_df_dropped_normalized.set_index(['state', 'year'], inplace=True)\n",
    "state_emissions_melted.set_index(['state', 'year'], inplace=True)\n",
    "\n",
    "final_df_dropped_normalized['emissions'] = state_emissions_melted['emissions']\n",
    "\n",
    "final_df_dropped_normalized.reset_index(inplace=True)\n",
    "state_emissions_melted.reset_index(inplace=True)\n",
    "\n",
    "final_df_dropped_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emissions initially declined, then rose steadily until the early 2000s, after which they began a continuous downward trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_by_year = final_df_imputed_normalized.groupby('year')['emissions'].mean().reset_index()\n",
    "print(emissions_by_year)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(emissions_by_year['year'], emissions_by_year['emissions'], marker='o')\n",
    "plt.title('Annual Emissions Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Emissions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_yearly_mean = final_df_imputed_normalized.groupby('year')['emissions'].mean().rename('overall_mean')\n",
    "\n",
    "overall_yearly_mean = overall_yearly_mean.sort_index()\n",
    "yearly_changes = overall_yearly_mean.diff().dropna()\n",
    "average_yearly_change = yearly_changes.abs().mean()\n",
    "average_total_emissions = overall_yearly_mean.mean()\n",
    "change_percentage = (average_yearly_change / average_total_emissions) * 100\n",
    "\n",
    "print(f\"Average absolute change in national emissions year-over-year: {average_yearly_change:.4f}\")\n",
    "print(f\"Average total emissions per year: {average_total_emissions:.4f}\")\n",
    "print(f\"Change as percentage of average total emissions: {change_percentage:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(yearly_changes.index, yearly_changes.abs(), marker='o')\n",
    "plt.title('Absolute Year-over-Year Change in National Emissions')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Absolute Change in Emissions')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframes are:\n",
    "#   1. final_df_dropped_normalized\n",
    "#   2. final_df_dropped\n",
    "#   3. final_df_imputed_normalized\n",
    "#   4. final_df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df_imputed.drop(columns=['Policy Scores'])\n",
    "final_df_normalized = final_df_imputed_normalized.drop(columns=['Policy Scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modified_normalized = final_df_normalized.copy()\n",
    "df_modified_normalized['emissions_per_capita'] = df_modified_normalized['emissions'] / df_modified_normalized['population']\n",
    "df_modified_normalized['population_density'] = df_modified_normalized['population'] / df_modified_normalized['size']\n",
    "df_modified_normalized = df_modified_normalized.drop(columns=['population', 'size', 'emissions'])\n",
    "\n",
    "df_modified = final_df.copy()\n",
    "df_modified['emissions_per_capita'] = df_modified['emissions'] / df_modified['population']\n",
    "df_modified['population_density'] = df_modified['population'] / df_modified['size']\n",
    "df_modified = df_modified.drop(columns=['population', 'size', 'emissions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjacency Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = (final_df_normalized[['state', 'population', 'size']]\n",
    "           .groupby('state', as_index=False)\n",
    "           .mean()\n",
    "           .rename(columns={'size': 'area'}))     # <-- area ≡ land‑size\n",
    "\n",
    "pop      = np.log(meta_df['population'].values)   # log‑scale as in Eq.(1)\n",
    "area     = meta_df['area'].values                # already in sq‑miles\n",
    "states   = meta_df['state'].tolist()\n",
    "N        = len(states)\n",
    "\n",
    "pop_z    = (pop - pop.mean()) / pop.std(ddof=0)\n",
    "area_z   = (area - area.mean()) / area.std(ddof=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "BORDERS = {  # Rook contiguity (lower‑48 + DC); AK & HI left empty\n",
    "    'AL':['FL','GA','MS','TN'], 'AZ':['CA','NV','UT','NM','CO'], 'AR':['LA','MO','MS','OK','TN','TX'],\n",
    "    'CA':['AZ','NV','OR'], 'CO':['AZ','KS','NM','NE','OK','UT','WY'], 'CT':['MA','NY','RI'],\n",
    "    'DC':['MD','VA'], 'DE':['MD','NJ','PA'], 'FL':['AL','GA'], 'GA':['AL','FL','NC','SC','TN'],\n",
    "    'HI':[], 'ID':['MT','NV','OR','UT','WA','WY'], 'IL':['IA','IN','KY','MO','WI'],\n",
    "    'IN':['IL','KY','MI','OH'], 'IA':['IL','MN','MO','NE','SD','WI'], 'KS':['CO','MO','NE','OK'],\n",
    "    'KY':['IL','IN','MO','OH','TN','VA','WV'], 'LA':['AR','MS','TX'], 'ME':['NH'],\n",
    "    'MD':['DC','DE','PA','VA','WV'], 'MA':['CT','NH','NY','RI','VT'],\n",
    "    'MI':['IN','OH','WI','MN'],  # upper‑peninsula touch via water ignored\n",
    "    'MN':['IA','ND','SD','WI'], 'MS':['AL','AR','LA','TN'], 'MO':['AR','IA','IL','KS','KY','NE','OK','TN'],\n",
    "    'MT':['ID','ND','SD','WY'], 'NE':['CO','IA','KS','MO','SD','WY'], 'NV':['AZ','CA','ID','OR','UT'],\n",
    "    'NH':['MA','ME','VT'], 'NJ':['DE','NY','PA'], 'NM':['AZ','CO','OK','TX','UT'],\n",
    "    'NY':['CT','MA','NJ','PA','VT'], 'NC':['GA','SC','TN','VA'], 'ND':['MN','MT','SD'],\n",
    "    'OH':['IN','KY','MI','PA','WV'], 'OK':['AR','CO','KS','MO','NM','TX'], 'OR':['CA','ID','NV','WA'],\n",
    "    'PA':['DE','MD','NJ','NY','OH','WV'], 'RI':['CT','MA'], 'SC':['GA','NC'],\n",
    "    'SD':['IA','MN','MT','ND','NE','WY'], 'TN':['AL','AR','GA','KY','MO','MS','NC','VA'],\n",
    "    'TX':['AR','LA','NM','OK'], 'UT':['AZ','CO','ID','NV','NM','WY'],\n",
    "    'VT':['MA','NH','NY'], 'VA':['DC','KY','MD','NC','TN','WV'], 'WA':['ID','OR'],\n",
    "    'WV':['KY','MD','OH','PA','VA'], 'WI':['IA','IL','MI','MN'], 'WY':['CO','ID','MT','NE','SD','UT']\n",
    "}\n",
    "\n",
    "C = np.zeros((N, N), dtype=int)\n",
    "for i, s in enumerate(states):\n",
    "    for nb in BORDERS.get(s, []):\n",
    "        if nb in states:                           # ignore AK–HI neighbours\n",
    "            j = states.index(nb)\n",
    "            C[i, j] = C[j, i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "γ1, γ2 = 1.0, 1.0          # <<< put your grid‑search loop around these\n",
    "eps    = 1e-4\n",
    "\n",
    "W = np.full((N, N), eps)\n",
    "for i, j in combinations(range(N), 2):\n",
    "    if C[i, j]:\n",
    "        diff_pop  = abs(pop_z[i]  - pop_z[j])\n",
    "        diff_area = abs(area_z[i] - area_z[j])\n",
    "        w_ij      = np.exp(-γ1*diff_pop**2 - γ2*diff_area**2)\n",
    "        W[i, j] = W[j, i] = w_ij\n",
    "np.fill_diagonal(W, 0.0)\n",
    "\n",
    "# Row‑stochastic (Eq. 2)\n",
    "W = W / W.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_graph = final_df_normalized.copy(deep=True)       \n",
    "\n",
    "state2row = dict(zip(states, range(N)))\n",
    "idx_ok    = final_df_graph['state'].isin(state2row)   # drop AK / HI if absent\n",
    "final_df_graph = final_df_graph.loc[idx_ok].reset_index(drop=True)\n",
    "\n",
    "graph_cols = ['pop_g0','area_g0','pop_g1','area_g1','pop_g2','area_g2']\n",
    "final_df_graph[graph_cols] = final_df_graph['state'].map(\n",
    "    lambda s: pd.Series(X_graph[state2row[s]]))\n",
    "\n",
    "print(\"Shape  w/out‑graph:\", final_df_normalized.shape)\n",
    "print(\"Shape with‑graph:\", final_df_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_regression(X, y, W_rw, lamb):\n",
    "    D  = np.diag(W_rw.sum(axis=1))\n",
    "    L  = D - W_rw\n",
    "    A  = X.T @ X + lamb * (X.T @ L @ X)\n",
    "    b  = X.T @ y\n",
    "    return np.linalg.solve(A, b)\n",
    "\n",
    "latest_year = final_df['year'].max()\n",
    "panel_t     = final_df[final_df['year'] == latest_year]\n",
    "X_design    = panel_t[['emissions',                         #   <- lag‑10 …\n",
    "                       'pop_g0','area_g0','pop_g1','area_g1','pop_g2','area_g2']].to_numpy()\n",
    "y_target    = panel_t['emissions'].to_numpy()\n",
    "β_hat       = laplacian_regression(X_design, y_target, W, lamb=0.3)\n",
    "rmse_graph  = np.sqrt(((X_design @ β_hat - y_target) ** 2).mean())\n",
    "print(f\"Graph‑regularised RMSE ({latest_year:.0f}) = {rmse_graph:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression_forecasting(df, window_size=3, n_splits=5, use_ridge=False, ridge_alpha=1.0):\n",
    "    try:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "        df = df.sort_values(['state', 'year'])\n",
    "\n",
    "        if 'state' not in df.columns or 'emissions' not in df.columns:\n",
    "            print(\"Required columns ('state', 'emissions') are missing.\")\n",
    "            return\n",
    "\n",
    "        lagged_dfs = []\n",
    "        for lag in range(1, window_size + 1):\n",
    "            lagged = df[['state', 'year', 'emissions']].copy()\n",
    "            lagged['year'] += lag\n",
    "            lagged.rename(columns={'emissions': f'emissions_t_minus_{lag}'}, inplace=True)\n",
    "            lagged_dfs.append(lagged)\n",
    "\n",
    "        for lagged in lagged_dfs:\n",
    "            df = df.merge(lagged, on=['state', 'year'], how='inner')\n",
    "\n",
    "        df = df.dropna()\n",
    "        if df.empty:\n",
    "            print(\"No data after sliding window merge.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Shape after sliding window processing: {df.shape}\")\n",
    "\n",
    "        features = [f'emissions_t_minus_{i}' for i in range(window_size, 0, -1)] + \\\n",
    "                   ['population', 'size', 'party_affiliation_score']\n",
    "        X = df[features]\n",
    "        y = df['emissions']\n",
    "        states = df['state']\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        r2_scores, mse_scores, explained_scores = [], [], []\n",
    "        coefs_all_folds = []\n",
    "        top_feature_counts = []\n",
    "        top_feature_fold_counts = []  # New: track top feature per fold\n",
    "        all_r2_by_state = []\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            states_test = states.iloc[test_index]\n",
    "\n",
    "            model = Ridge(alpha=ridge_alpha, normalize=True) if use_ridge else LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "            print(f\"Fold {fold} — R²: {r2:.4f}, MSE: {mse:.4f}, Explained Variance: {explained_var:.4f}\")\n",
    "            r2_scores.append(r2)\n",
    "            mse_scores.append(mse)\n",
    "            explained_scores.append(explained_var)\n",
    "            coefs_all_folds.append(model.coef_)\n",
    "\n",
    "            # New: track top feature for the fold\n",
    "            top_feature_fold = pd.Series(model.coef_, index=features).abs().idxmax()\n",
    "            top_feature_fold_counts.append(top_feature_fold)\n",
    "\n",
    "            plt.scatter(y_test, y_pred, label=f'Fold {fold}', alpha=0.6)\n",
    "\n",
    "            fold_df = df.iloc[test_index].copy()\n",
    "            fold_df['predicted'] = y_pred\n",
    "            fold_r2 = fold_df.groupby('state').apply(lambda g: r2_score(g['emissions'], g['predicted']))\n",
    "            all_r2_by_state.append(fold_r2)\n",
    "\n",
    "            for state, group in fold_df.groupby('state'):\n",
    "                if len(group) < window_size + 3:\n",
    "                    continue\n",
    "                X_state = group[features]\n",
    "                y_state = group['emissions']\n",
    "                model_state = Ridge(alpha=ridge_alpha, normalize=True) if use_ridge else LinearRegression()\n",
    "                model_state.fit(X_state, y_state)\n",
    "                coefs = pd.Series(model_state.coef_, index=features).abs()\n",
    "                top_feature = coefs.idxmax()\n",
    "                top_feature_counts.append(top_feature)\n",
    "\n",
    "        min_val, max_val = y.min(), y.max()\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Fit')\n",
    "        plt.xlabel('Actual Emissions')\n",
    "        plt.ylabel('Predicted Emissions')\n",
    "        plt.title('Ridge Regression' if use_ridge else 'Linear Regression')\n",
    "        plt.legend(title='Fold')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nAverage R² Score: {np.mean(r2_scores):.4f}\")\n",
    "        print(f\"Average Mean Squared Error: {np.mean(mse_scores):.4f}\")\n",
    "        print(f\"Average Explained Variance Score: {np.mean(explained_scores):.4f}\")\n",
    "\n",
    "        coef_matrix = np.array(coefs_all_folds)\n",
    "        mean_coefs = coef_matrix.mean(axis=0)\n",
    "        std_coefs = coef_matrix.std(axis=0)\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Mean Coefficient': mean_coefs,\n",
    "            'Std Deviation': std_coefs\n",
    "        }).sort_values(by='Mean Coefficient', key=abs, ascending=False)\n",
    "\n",
    "        print(\"\\nAverage Feature Coefficients Across All Folds:\")\n",
    "        print(coef_df)\n",
    "\n",
    "        r2_by_state_all_folds = pd.concat(all_r2_by_state, axis=1).mean(axis=1)\n",
    "        stratified_r2 = r2_by_state_all_folds.mean()\n",
    "\n",
    "        print(f\"\\nStratified R² (average across states): {stratified_r2:.4f}\")\n",
    "        print(f\"Highest R² State: {r2_by_state_all_folds.idxmax()} ({r2_by_state_all_folds.max():.4f})\")\n",
    "        print(f\"Lowest R² State: {r2_by_state_all_folds.idxmin()} ({r2_by_state_all_folds.min():.4f})\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        r2_by_state_all_folds.sort_values(ascending=False).plot(kind='bar')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.title('R² by State (Averaged Across Folds)')\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('R² Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        percent_party_top = (sum(f == 'party_affiliation_score' for f in top_feature_counts) / len(top_feature_counts)) * 100\n",
    "        print(f\"\\nPercentage of state models (across all folds) where 'party_affiliation_score' was the top predictor: {percent_party_top:.2f}%\")\n",
    "\n",
    "        percent_party_top_fold = (sum(f == 'party_affiliation_score' for f in top_feature_fold_counts) / len(top_feature_fold_counts)) * 100\n",
    "        print(f\"Percentage of folds where 'party_affiliation_score' was the top predictor: {percent_party_top_fold:.2f}%\")\n",
    "\n",
    "        return (\n",
    "            r2_scores,\n",
    "            mse_scores,\n",
    "            explained_scores,\n",
    "            coef_df,\n",
    "            stratified_r2,\n",
    "            r2_by_state_all_folds.sort_values(ascending=False),\n",
    "            percent_party_top,\n",
    "            percent_party_top_fold\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores, mse_scores, explained_scores, coef_df, stratified_r2, r2_by_state, percent_party_top, percent_party_top_fold = train_linear_regression_forecasting(final_df_normalized, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores, mse_scores, explained_scores, coef_df, stratified_r2, r2_by_state, percent_party_top, percent_party_top_fold = train_linear_regression_forecasting(final_df_normalized, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_scores, mse_scores, explained_scores, coef_df, stratified_r2, r2_by_state, percent_party_top, percent_party_top_fold = train_linear_regression_forecasting(\n",
    "    final_df_graph,\n",
    "    window_size=10,\n",
    "    extra_graph_features=['pop_g0','area_g0','pop_g1','area_g1','pop_g2','area_g2']\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_linear_regression_forecasting(final_df_normalized, window_size=10, n_splits=5, use_ridge=True, ridge_alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_linear_regression_forecasting(final_df_normalized, window_size=5, n_splits=5, use_ridge=True, ridge_alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cols = ['pop_g0','area_g0','pop_g1','area_g1','pop_g2','area_g2']\n",
    "ridge_graph_10 = train_linear_regression_forecasting(\n",
    "    final_df_graph,           \n",
    "    window_size=10,\n",
    "    n_splits=5,\n",
    "    use_ridge=True,\n",
    "    ridge_alpha=0.5,\n",
    "    extra_graph_features=graph_cols  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_graph_5 = train_linear_regression_forecasting(\n",
    "    final_df_graph,           \n",
    "    window_size=5,\n",
    "    n_splits=5,\n",
    "    use_ridge=True,\n",
    "    ridge_alpha=0.5,\n",
    "    extra_graph_features=graph_cols  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_normalized_copy = final_df_normalized\n",
    "\n",
    "final_df_normalized_copy['emissions_change'] = (\n",
    "    final_df_normalized_copy\n",
    "    .sort_values(['state', 'year'])\n",
    "    .groupby('state')['emissions']\n",
    "    .diff()\n",
    ")\n",
    "\n",
    "state_means = final_df_normalized_copy.drop(columns=['year']).groupby('state').mean().reset_index()\n",
    "r2_df = r2_by_state.reset_index()\n",
    "r2_df.columns = ['state', 'r2_score']\n",
    "merged_df = state_means.merge(r2_df, on='state', how='inner')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_r2_clusters_from_merged(merged_df, max_k=10):\n",
    "    try:\n",
    "        clustering_features = ['r2_score', 'party_affiliation_score', 'emissions_change', 'population', 'size', 'emissions']\n",
    "\n",
    "        for col in clustering_features:\n",
    "            merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "\n",
    "        merged_df[clustering_features] = merged_df[clustering_features].fillna(\n",
    "            merged_df[clustering_features].mean()\n",
    "        )\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(merged_df[clustering_features])\n",
    "\n",
    "        sse = []\n",
    "        K_range = range(1, min(len(X_scaled), max_k) + 1)\n",
    "        for k in K_range:\n",
    "            km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            km.fit(X_scaled)\n",
    "            sse.append(km.inertia_)\n",
    "\n",
    "        kn = KneeLocator(K_range, sse, curve='convex', direction='decreasing')\n",
    "        optimal_k = kn.elbow if kn.elbow is not None else 3  # fallback if not found\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(K_range, sse, marker='o')\n",
    "        if kn.elbow:\n",
    "            plt.axvline(x=kn.elbow, color='red', linestyle='--', label=f'Elbow at k={kn.elbow}')\n",
    "            plt.legend()\n",
    "        plt.title('Elbow Method to Determine Optimal Clusters')\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('SSE (Inertia)')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"✅ Optimal number of clusters selected: {optimal_k}\")\n",
    "\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        merged_df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "        cluster_summary = merged_df.groupby('cluster')[clustering_features].mean().round(3)\n",
    "        feature_ranges = cluster_summary.max() - cluster_summary.min()\n",
    "        top_discriminative_features = feature_ranges.sort_values(ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=merged_df,\n",
    "            x='party_affiliation_score',\n",
    "            y='r2_score',\n",
    "            hue='cluster',\n",
    "            palette='viridis',\n",
    "            s=100\n",
    "        )\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.title('State R² vs. Party Affiliation Score (Clustered)')\n",
    "        plt.xlabel('Party Affiliation Score')\n",
    "        plt.ylabel('R² Score')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nCluster Summary:\")\n",
    "        print(cluster_summary)\n",
    "\n",
    "        print(\"\\nTop Discriminative Features Across Clusters:\")\n",
    "        print(top_discriminative_features)\n",
    "\n",
    "        return merged_df, cluster_summary, top_discriminative_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred in clustering: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df, summary_df, top_features = analyze_r2_clusters_from_merged(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_normalized_tmp = final_df_normalized_copy\n",
    "final_df_normalized_tmp['political_party'] = final_df_normalized['party_affiliation_score'].apply(lambda x: 'D' if x > 0 else 'R')\n",
    "scaler = StandardScaler()\n",
    "final_df_normalized_tmp['emissions'] = scaler.fit_transform(final_df_normalized_tmp[['emissions']])\n",
    "final_df_normalized_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_change_by_party(df, value_column='emissions', party_column='political_party'):\n",
    "    df = df.sort_values(['state', 'year'])\n",
    "    df['value_diff'] = df.groupby('state')[value_column].diff()\n",
    "    state_avg_change = df.groupby('state')['value_diff'].mean().reset_index()\n",
    "    party_map = df.drop_duplicates('state')[['state', party_column]]\n",
    "    merged = state_avg_change.merge(party_map, on='state', how='left')\n",
    "    result = merged.groupby(party_column)['value_diff'].mean().reset_index().rename(columns={'value_diff': 'avg_change'})\n",
    "    return result\n",
    "\n",
    "average_change_by_party(final_df_normalized_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = final_df_normalized_tmp['political_party'].value_counts(normalize=True) * 100\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_forecasting(\n",
    "    df,\n",
    "    window_size=3,\n",
    "    n_splits=5,\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    exclude_demo_features=False,\n",
    "    shuffle_labels=True\n",
    "):\n",
    "    try:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "        df = df.sort_values(['state', 'year'])\n",
    "\n",
    "        if 'state' not in df.columns or 'political_party' not in df.columns:\n",
    "            print(\"Required columns ('state', 'political_party') are missing.\")\n",
    "            return\n",
    "\n",
    "        lagged_dfs = []\n",
    "        for lag in range(1, window_size + 1):\n",
    "            lagged = df[['state', 'year', 'emissions']].copy()\n",
    "            lagged['year'] += lag\n",
    "            lagged.rename(columns={'emissions': f'emissions_t_minus_{lag}'}, inplace=True)\n",
    "            lagged_dfs.append(lagged)\n",
    "\n",
    "        for lagged in lagged_dfs:\n",
    "            df = df.merge(lagged, on=['state', 'year'], how='inner')\n",
    "\n",
    "        df = df.dropna()\n",
    "        if df.empty:\n",
    "            print(\"No data after sliding window merge.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Shape after sliding window processing: {df.shape}\")\n",
    "\n",
    "        base_features = [f'emissions_t_minus_{i}' for i in range(window_size, 0, -1)]\n",
    "        extra_features = ['population', 'size']\n",
    "        features = base_features + ([] if exclude_demo_features else extra_features)\n",
    "\n",
    "        X = df[features]\n",
    "        y = df['political_party'].copy()\n",
    "        states = df['state']\n",
    "\n",
    "        if shuffle_labels:\n",
    "            print(\"Shuffling political_party labels to test against random baseline...\")\n",
    "            y = y.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        accuracy_scores = []\n",
    "        precision_scores = []\n",
    "        recall_scores = []\n",
    "        f1_scores = []\n",
    "        coefs_all_folds = []\n",
    "        top_feature_counts = []\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            states_test = states.iloc[test_index]\n",
    "\n",
    "            if len(np.unique(y_train)) < 2:\n",
    "                print(f\"Fold {fold} skipped — only one class ({np.unique(y_train)[0]}) present in training data.\")\n",
    "                continue\n",
    "\n",
    "            model = LogisticRegression(penalty=penalty, C=C, max_iter=1000)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            prec = precision_score(y_test, y_pred, pos_label='R', zero_division=0)\n",
    "            rec = recall_score(y_test, y_pred, pos_label='R', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, pos_label='R', zero_division=0)\n",
    "\n",
    "            accuracy_scores.append(acc)\n",
    "            precision_scores.append(prec)\n",
    "            recall_scores.append(rec)\n",
    "            f1_scores.append(f1)\n",
    "            coefs_all_folds.append(model.coef_[0])\n",
    "\n",
    "            print(f\"Fold {fold} — Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "            plt.scatter(range(len(y_test)), y_pred, alpha=0.6, label=f'Fold {fold}')\n",
    "\n",
    "            fold_df = df.iloc[test_index].copy()\n",
    "            for state, group in fold_df.groupby('state'):\n",
    "                if len(group) < window_size + 3:\n",
    "                    continue\n",
    "                X_state = group[features]\n",
    "                y_state = y.iloc[group.index]\n",
    "                if len(np.unique(y_state)) < 2:\n",
    "                    continue\n",
    "                model_state = LogisticRegression(penalty=penalty, C=C, max_iter=1000)\n",
    "                model_state.fit(X_state, y_state)\n",
    "                coefs = pd.Series(model_state.coef_[0], index=features).abs()\n",
    "                top_feature = coefs.idxmax()\n",
    "                top_feature_counts.append(top_feature)\n",
    "\n",
    "            if fold == n_splits:\n",
    "                final_test_df = pd.DataFrame({\n",
    "                    'state': states_test.values,\n",
    "                    'actual': y_test.values,\n",
    "                    'predicted': y_pred\n",
    "                })\n",
    "\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Predicted Political Party')\n",
    "        plt.title('Logistic Regression Predictions by Fold')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if not accuracy_scores:\n",
    "            print(\"\\nNo valid folds with both classes. Cannot compute overall metrics.\")\n",
    "            return None\n",
    "\n",
    "        print(f\"\\nAverage Accuracy Across Folds:  {np.mean(accuracy_scores):.4f}\")\n",
    "        print(f\"Average Precision (R):          {np.mean(precision_scores):.4f}\")\n",
    "        print(f\"Average Recall (R):             {np.mean(recall_scores):.4f}\")\n",
    "        print(f\"Average F1 Score (R):           {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "        coef_matrix = np.array(coefs_all_folds)\n",
    "        mean_coefs = coef_matrix.mean(axis=0)\n",
    "        std_coefs = coef_matrix.std(axis=0)\n",
    "\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Mean Coefficient': mean_coefs,\n",
    "            'Std Deviation': std_coefs\n",
    "        }).sort_values(by='Mean Coefficient', key=abs, ascending=False)\n",
    "\n",
    "        print(\"\\nAverage Feature Coefficients Across All Folds:\")\n",
    "        print(coef_df)\n",
    "\n",
    "        non_lag_df = coef_df[~coef_df['Feature'].str.startswith('emissions_t_minus_')]\n",
    "        print(\"\\nNon-Lag Feature Coefficients (Excludes emissions_t_minus_X):\")\n",
    "        print(non_lag_df)\n",
    "\n",
    "        accuracy_by_state = final_test_df.groupby('state').apply(lambda g: accuracy_score(g['actual'], g['predicted']))\n",
    "        stratified_accuracy = accuracy_by_state.mean()\n",
    "\n",
    "        print(f\"\\nStratified Accuracy (average across states): {stratified_accuracy:.4f}\")\n",
    "        print(f\"Highest Accuracy State: {accuracy_by_state.idxmax()} ({accuracy_by_state.max():.4f})\")\n",
    "        print(f\"Lowest Accuracy State: {accuracy_by_state.idxmin()} ({accuracy_by_state.min():.4f})\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        accuracy_by_state.sort_values(ascending=False).plot(kind='bar')\n",
    "        plt.axhline(0.5, color='red', linestyle='--')\n",
    "        plt.title('Accuracy by State (Last Fold)')\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        total_state_models = len(top_feature_counts)\n",
    "        top_feature_counts_series = pd.Series(top_feature_counts)\n",
    "        top_feature_counts_percent = top_feature_counts_series.value_counts(normalize=True) * 100\n",
    "\n",
    "        print(\"\\nTop Feature Frequency (Percent of States Where Each Feature Was Most Predictive):\")\n",
    "        print(top_feature_counts_percent)\n",
    "\n",
    "        return (\n",
    "            accuracy_scores,\n",
    "            coef_df,\n",
    "            non_lag_df,\n",
    "            stratified_accuracy,\n",
    "            accuracy_by_state.sort_values(ascending=False),\n",
    "            top_feature_counts_percent\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logistic_regression_forecasting(final_df_normalized_tmp, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logistic_regression_forecasting(final_df_normalized_tmp, window_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineered Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def train_feature_engineered_lr(df, window_size=3, n_splits=5, use_ridge=False, ridge_alpha=1.0):\n",
    "    try:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "        df = df.sort_values(['state', 'year'])\n",
    "\n",
    "        if 'state' not in df.columns or 'emissions_per_capita' not in df.columns:\n",
    "            print(\"Required columns ('state', 'emissions_per_capita') are missing.\")\n",
    "            return\n",
    "\n",
    "        # Create lagged emissions_per_capita features\n",
    "        lagged_dfs = []\n",
    "        for lag in range(1, window_size + 1):\n",
    "            lagged = df[['state', 'year', 'emissions_per_capita']].copy()\n",
    "            lagged['year'] += lag\n",
    "            lagged.rename(columns={'emissions_per_capita': f'emissions_t_minus_{lag}'}, inplace=True)\n",
    "            lagged_dfs.append(lagged)\n",
    "\n",
    "        for lagged in lagged_dfs:\n",
    "            df = df.merge(lagged, on=['state', 'year'], how='inner')\n",
    "\n",
    "        df = df.dropna()\n",
    "        if df.empty:\n",
    "            print(\"No data after sliding window merge.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Shape after sliding window processing: {df.shape}\")\n",
    "\n",
    "        features = [f'emissions_t_minus_{i}' for i in range(window_size, 0, -1)] + ['population_density']\n",
    "        X = df[features]\n",
    "        y = df['party_affiliation_score']\n",
    "        states = df['state']\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        r2_scores, mse_scores, explained_scores = [], [], []\n",
    "        coefs_all_folds, top_feature_counts = [], []\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for fold, (train_idx, test_idx) in enumerate(tscv.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            states_test = states.iloc[test_idx]\n",
    "\n",
    "            model = Ridge(alpha=ridge_alpha) if use_ridge else LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "            print(f\"Fold {fold} — R²: {r2:.4f}, MSE: {mse:.4f}, Explained Variance: {explained_var:.4f}\")\n",
    "            r2_scores.append(r2)\n",
    "            mse_scores.append(mse)\n",
    "            explained_scores.append(explained_var)\n",
    "            coefs_all_folds.append(model.coef_)\n",
    "\n",
    "            plt.scatter(y_test, y_pred, label=f'Fold {fold}', alpha=0.6)\n",
    "\n",
    "            # Feature importance tracking per state\n",
    "            fold_df = df.iloc[test_idx].copy()\n",
    "            for state, group in fold_df.groupby('state'):\n",
    "                if len(group) < window_size + 3:\n",
    "                    continue\n",
    "                X_state = group[features]\n",
    "                y_state = group['party_affiliation_score']\n",
    "                model_state = Ridge(alpha=ridge_alpha) if use_ridge else LinearRegression()\n",
    "                model_state.fit(X_state, y_state)\n",
    "                coefs = pd.Series(model_state.coef_, index=features).abs()\n",
    "                top_feature = coefs.idxmax()\n",
    "                top_feature_counts.append(top_feature)\n",
    "\n",
    "            if fold == n_splits:\n",
    "                final_test_df = pd.DataFrame({\n",
    "                    'state': states_test.values,\n",
    "                    'actual': y_test.values,\n",
    "                    'predicted': y_pred\n",
    "                })\n",
    "\n",
    "        min_val, max_val = min(y.min(), y.max()), max(y.min(), y.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Fit')\n",
    "        plt.xlabel('Actual Party Affiliation Score')\n",
    "        plt.ylabel('Predicted Score')\n",
    "        plt.title('Feature Engineered Linear Regression' + (' (Ridge)' if use_ridge else ''))\n",
    "        plt.legend(title='Fold')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nAverage R² Score: {np.mean(r2_scores):.4f}\")\n",
    "        print(f\"Average Mean Squared Error: {np.mean(mse_scores):.4f}\")\n",
    "        print(f\"Average Explained Variance Score: {np.mean(explained_scores):.4f}\")\n",
    "\n",
    "        coef_matrix = np.array(coefs_all_folds)\n",
    "        mean_coefs = coef_matrix.mean(axis=0)\n",
    "        std_coefs = coef_matrix.std(axis=0)\n",
    "\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Mean Coefficient': mean_coefs,\n",
    "            'Std Deviation': std_coefs\n",
    "        }).sort_values(by='Mean Coefficient', key=abs, ascending=False)\n",
    "\n",
    "        print(\"\\nAverage Feature Coefficients Across All Folds:\")\n",
    "        print(coef_df)\n",
    "\n",
    "        r2_by_state = final_test_df.groupby('state').apply(lambda g: r2_score(g['actual'], g['predicted']))\n",
    "        stratified_r2 = r2_by_state.mean()\n",
    "\n",
    "        print(f\"\\nStratified R² (average across states): {stratified_r2:.4f}\")\n",
    "        print(f\"Highest R² State: {r2_by_state.idxmax()} ({r2_by_state.max():.4f})\")\n",
    "        print(f\"Lowest R² State: {r2_by_state.idxmin()} ({r2_by_state.min():.4f})\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        r2_by_state.sort_values(ascending=False).plot(kind='bar')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.title('R² by State (Last Fold)')\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('R² Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        total_state_models = len(top_feature_counts)\n",
    "        top_emissions_pct = (sum(f.startswith('emissions') for f in top_feature_counts) / total_state_models) * 100\n",
    "\n",
    "        print(f\"\\nPercentage of state models where an 'emissions_t_minus_X' feature was most predictive: {top_emissions_pct:.2f}%\")\n",
    "\n",
    "        return r2_scores, mse_scores, explained_scores, coef_df, stratified_r2, r2_by_state.sort_values(ascending=False), top_emissions_pct\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_feature_engineered_lr(df_modified_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_tmp = final_df\n",
    "final_df_tmp['political_party'] = final_df['party_affiliation_score'].apply(lambda x: 'D' if x > 0 else 'R')\n",
    "final_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def train_random_forest_forecasting(df, window_size=5, n_splits=5, n_estimators=100, extra_graph_features=None):\n",
    "    extra_graph_features = extra_graph_features or []\n",
    "    try:\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "        df = df.sort_values(['state', 'year'])\n",
    "\n",
    "        if 'state' not in df.columns or 'emissions' not in df.columns:\n",
    "            print(\"Required columns ('state', 'emissions') are missing.\")\n",
    "            return\n",
    "\n",
    "        lagged_dfs = []\n",
    "        for lag in range(1, window_size + 1):\n",
    "            lagged = df[['state', 'year', 'emissions']].copy()\n",
    "            lagged['year'] += lag\n",
    "            lagged.rename(columns={'emissions': f'emissions_t_minus_{lag}'}, inplace=True)\n",
    "            lagged_dfs.append(lagged)\n",
    "\n",
    "        for lagged in lagged_dfs:\n",
    "            df = df.merge(lagged, on=['state', 'year'], how='inner')\n",
    "\n",
    "        df = df.dropna()\n",
    "        if df.empty:\n",
    "            print(\"No data after sliding window merge.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Shape after sliding window processing: {df.shape}\")\n",
    "\n",
    "        features = [f'emissions_t_minus_{i}' for i in range(window_size, 0, -1)] + \\\n",
    "                   ['population', 'size', 'party_affiliation_score'] + extra_graph_features   \n",
    "        X = df[features]\n",
    "        y = df['emissions']\n",
    "        states = df['state']\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        r2_scores, mse_scores, explained_scores = [], [], []\n",
    "        feature_importance_all_folds = []\n",
    "        top_feature_counts = []\n",
    "        all_r2_by_state = []\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            states_test = states.iloc[test_index]\n",
    "\n",
    "            model = RandomForestRegressor(n_estimators=n_estimators)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            explained_var = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "            print(f\"Fold {fold} — R²: {r2:.4f}, MSE: {mse:.4f}, Explained Variance: {explained_var:.4f}\")\n",
    "            r2_scores.append(r2)\n",
    "            mse_scores.append(mse)\n",
    "            explained_scores.append(explained_var)\n",
    "            feature_importance_all_folds.append(model.feature_importances_)\n",
    "\n",
    "            top_feature_fold = pd.Series(model.feature_importances_, index=features).idxmax()\n",
    "            top_feature_counts.append(top_feature_fold)\n",
    "\n",
    "            plt.scatter(y_test, y_pred, label=f'Fold {fold}', alpha=0.6)\n",
    "\n",
    "            fold_df = df.iloc[test_index].copy()\n",
    "            fold_df['predicted'] = y_pred\n",
    "            fold_r2 = fold_df.groupby('state').apply(lambda g: r2_score(g['emissions'], g['predicted']))\n",
    "            all_r2_by_state.append(fold_r2)\n",
    "\n",
    "        min_val, max_val = y.min(), y.max()\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Fit')\n",
    "        plt.xlabel('Actual Emissions')\n",
    "        plt.ylabel('Predicted Emissions')\n",
    "        plt.title('Random Forest Regression')\n",
    "        plt.legend(title='Fold')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nAverage R² Score: {np.mean(r2_scores):.4f}\")\n",
    "        print(f\"Average Mean Squared Error: {np.mean(mse_scores):.4f}\")\n",
    "        print(f\"Average Explained Variance Score: {np.mean(explained_scores):.4f}\")\n",
    "\n",
    "        # Feature importances\n",
    "        importances_matrix = np.array(feature_importance_all_folds)\n",
    "        mean_importances = importances_matrix.mean(axis=0)\n",
    "        std_importances = importances_matrix.std(axis=0)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': features,\n",
    "            'Mean Importance': mean_importances,\n",
    "            'Std Deviation': std_importances\n",
    "        }).sort_values(by='Mean Importance', ascending=False)\n",
    "\n",
    "        print(\"\\nAverage Feature Importances Across All Folds:\")\n",
    "        print(importance_df)\n",
    "\n",
    "        # Stratified R² by state\n",
    "        r2_by_state_all_folds = pd.concat(all_r2_by_state, axis=1).mean(axis=1)\n",
    "        stratified_r2 = r2_by_state_all_folds.mean()\n",
    "\n",
    "        print(f\"\\nStratified R² (average across states): {stratified_r2:.4f}\")\n",
    "        print(f\"Highest R² State: {r2_by_state_all_folds.idxmax()} ({r2_by_state_all_folds.max():.4f})\")\n",
    "        print(f\"Lowest R² State: {r2_by_state_all_folds.idxmin()} ({r2_by_state_all_folds.min():.4f})\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        r2_by_state_all_folds.sort_values(ascending=False).plot(kind='bar')\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.title('R² by State (Averaged Across Folds)')\n",
    "        plt.xlabel('State')\n",
    "        plt.ylabel('R² Score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        percent_party_top = (sum(f == 'party_affiliation_score' for f in top_feature_counts) / len(top_feature_counts)) * 100\n",
    "        print(f\"\\nPercentage of folds where 'party_affiliation_score' was the most important feature: {percent_party_top:.2f}%\")\n",
    "\n",
    "        return (\n",
    "            r2_scores,\n",
    "            mse_scores,\n",
    "            explained_scores,\n",
    "            importance_df,\n",
    "            stratified_r2,\n",
    "            r2_by_state_all_folds.sort_values(ascending=False),\n",
    "            percent_party_top\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest_forecasting(final_df_tmp, window_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest_forecasting(final_df_tmp, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest_forecasting(final_df_tmp, window_size=10, extra_graph_features = graph_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest_forecasting(final_df_tmp, window_size=5, extra_graph_features = graph_cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
